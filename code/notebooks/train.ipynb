{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juan-villa02/medical_vqa_vlm/blob/main/code/notebooks/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gIbSRmC_Q8o"
      },
      "source": [
        "# VQA Model - BERT + ResNet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnubYJEE_g7f"
      },
      "source": [
        "## Libraries/Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOxuYqd1_aMO"
      },
      "outputs": [],
      "source": [
        "# PyTorch framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as utils\n",
        "# Image and mask handling\n",
        "from PIL import Image\n",
        "from skimage import transform\n",
        "import scipy.io as sio\n",
        "# Transformers (Hugginface)\n",
        "from transformers import AutoTokenizer, BertForQuestionAnswering, VisualBertModel, VisualBertForQuestionAnswering, VisualBertConfig\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# Numpy\n",
        "import numpy as np\n",
        "# Extra dependencies\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d29Ogg0EBJgx"
      },
      "outputs": [],
      "source": [
        "# Set training device to GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5MQ2XccBctw"
      },
      "source": [
        "## Paths & Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bhmZStTCxv4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhz88OSQBaIZ"
      },
      "outputs": [],
      "source": [
        "path_dir = '.'\n",
        "path_dir = '/content/drive/MyDrive/TFG Juan Villanueva/databases_qa.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB-bgRNCoPqN"
      },
      "outputs": [],
      "source": [
        "# Data folder\n",
        "data_folder = './data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nuib-S0pXlU"
      },
      "outputs": [],
      "source": [
        "# ISIC_2016 database\n",
        "train_ISIC_path = 'databases_qa/ISIC_2016'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def filter_json(json_file, images_dir, output_file):\n",
        "#    filtered_data = []\n",
        "\n",
        "#    with open(json_file, 'r') as f:\n",
        "#        data = json.load(f)\n",
        "\n",
        "#        for item in data:\n",
        "#            image_id = item['image_id'] + '.jpg'\n",
        "#            image_path = os.path.join(images_dir, image_id)\n",
        "\n",
        "#            # Check if the image file exists\n",
        "#            if os.path.exists(image_path):\n",
        "#                filtered_data.append(item)\n",
        "\n",
        "#    # Write the filtered data to the output JSON file\n",
        "#    with open(output_file, 'w') as f:\n",
        "#        json.dump(filtered_data, f, indent=4)\n",
        "\n",
        "#    return"
      ],
      "metadata": {
        "id": "OdoT841ba95x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json_file = '/content/data/databases_qa/ISIC_2016/qa_ISIC.json'\n",
        "# images_dir = '/content/data/databases_qa/ISIC_2016/images/ISBI2016_ISIC_Part1_Test_Data_orig'\n",
        "# output_file = 'qa_filtered_ISIC.json'\n",
        "# filter_json(json_file, images_dir, output_file)"
      ],
      "metadata": {
        "id": "shV8YZa3bA5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ISIC_2016 database (images and masks)\n",
        "train_extra_ISIC_path = 'ISBI2016_ISIC_Part3_Training_Data_orig'"
      ],
      "metadata": {
        "id": "ZHU5hHJY-pDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMjnxi-_p38f"
      },
      "outputs": [],
      "source": [
        "# Pizarro database (images)\n",
        "pizarro_path = 'databases_qa/pizarro'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcY75F9to4XA"
      },
      "outputs": [],
      "source": [
        "# Function to extract databases from the main zip file\n",
        "def extract_databases(zip_path, extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XecsY9GIlCDx"
      },
      "outputs": [],
      "source": [
        "# Extract all databases from the main zip file\n",
        "if path_dir != '.':\n",
        "  extract_databases(path_dir, data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IenAqE-1rKgD"
      },
      "outputs": [],
      "source": [
        "# ISIC_2016 images\n",
        "database1_path = os.path.join(data_folder, train_ISIC_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfi5boA0s727"
      },
      "outputs": [],
      "source": [
        "# Pizarro images\n",
        "# 500 imágenes; Las 46 primeras son más complejas, en cuanto a diagnóstico. (Entrega 1) El resto están más equilibradas (Entrega 2-5)\n",
        "database2_path = os.path.join(data_folder, pizarro_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SedFsMzrAegB"
      },
      "source": [
        "## VQA Dataset - Images, Masks, & QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AUIrbp_w4CS"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load images and questions\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, data_dir, json_file, tokenizer=None, transform=None, extraPath=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.images_dir = os.path.join(self.data_dir, 'images')\n",
        "        self.masks_dir = os.path.join(self.data_dir, 'masks')\n",
        "\n",
        "        # Handle special ISIC data folder\n",
        "        self.extraPath = extraPath\n",
        "        if self.extraPath is not None:\n",
        "            self.images_dir = os.path.join(self.images_dir, self.extraPath)\n",
        "            self.masks_dir = os.path.join(self.masks_dir, self.extraPath)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        with open(json_file, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.images_dir, item['image_id'])\n",
        "\n",
        "        # Ensure the image ID ends with \".jpg\"\n",
        "        if not image_path.endswith(\".jpg\"):\n",
        "            image_path += \".jpg\"\n",
        "\n",
        "        # Check if the image is available\n",
        "        try:\n",
        "          image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "          # Case 1: Pizarro dataset\n",
        "          if self.extraPath is None:\n",
        "            mask_id = item['image_id'].split('.')[0]\n",
        "            mask_name = 'L' + mask_id + '.pgm'\n",
        "            mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "            mask = Image.open(mask_path)\n",
        "\n",
        "          # Case 2: ISIC dataset\n",
        "          else:\n",
        "            mask_path = os.path.join(self.masks_dir, item['image_id']) + '_Segmentation.png'\n",
        "            mask = Image.open(mask_path)\n",
        "\n",
        "          if self.transform:\n",
        "              image, mask = self.transform((image, mask))\n",
        "\n",
        "          # Store questions and answers in two different arrays\n",
        "          qa_pairs = item['qa_pairs']\n",
        "          questions = [qa['question'] for qa in qa_pairs]\n",
        "          answers = [qa['answer'] for qa in qa_pairs]\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "\n",
        "          image = None\n",
        "          mask = None\n",
        "          questions = None\n",
        "          answers = None\n",
        "\n",
        "        return image, mask, questions, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV9ivOizwN-r"
      },
      "outputs": [],
      "source": [
        "ISIC_Dataset = VQADataset(database1_path,'/content/data/databases_qa/ISIC_2016/qa_filtered_ISIC.json', tokenizer=None, transform=None, extraPath=train_extra_ISIC_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b45i5PCxoA5"
      },
      "outputs": [],
      "source": [
        "ISIC_Dataset[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "---H6oXn4860"
      },
      "source": [
        "## Load pre-trained ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXzT4ZfT5BTU"
      },
      "outputs": [],
      "source": [
        "# Load ResNet-18 model from .pth file\n",
        "def load_resnet(path, optimizer=None, scheduler=None):\n",
        "    resnet = models.resnet18()\n",
        "    # Remove final fully connected layer (we do not want the model to return labels but features)\n",
        "    resnet.fc = nn.Identity()\n",
        "\n",
        "    # Load the weights\n",
        "    state_dict = torch.load(path, map_location=device)['state_dict']\n",
        "\n",
        "    # Remove unexpected keys if they exist\n",
        "    unexpected_keys = ['fc.weight', 'fc.bias']\n",
        "    for key in unexpected_keys:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "\n",
        "    # Load the weights\n",
        "    resnet.load_state_dict(state_dict)\n",
        "    resnet.eval()\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(torch.load(path)['optimizer'])\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(torch.load(path)['scheduler'])\n",
        "\n",
        "    resnet.to(device)\n",
        "\n",
        "    return resnet, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rGRJcyj5LKz"
      },
      "outputs": [],
      "source": [
        "model_source_path = '/content/drive/MyDrive/TFG Juan Villanueva/codigo diagnostico/bestdiagNet.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm9XDY285UOB"
      },
      "outputs": [],
      "source": [
        "resnet, optimizer, scheduler = load_resnet(model_source_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCtvmxrn9m2u"
      },
      "outputs": [],
      "source": [
        "resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esJtbykJp-f4"
      },
      "source": [
        "## Image Transformations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CropByMask(object):\n",
        "    \"\"\"Recortamos la imagen usando la máscara de la lesión.\n",
        "\n",
        "    Args:\n",
        "        border (tupla o int): El borde de recorte alrededor de la máscara. Es sabido que el análisis del borde\n",
        "        de la lesión con la piel circudante es importante para los dermatólogos, por lo que puede ser interesante\n",
        "        dejar una guarda.\n",
        "        Si es una tupla, entonces es (bordery,borderx)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, border):\n",
        "        assert isinstance(border, (int, tuple))\n",
        "        if isinstance(border, int):\n",
        "            self.border = (border,border)\n",
        "        else:\n",
        "            self.border = border\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        image, mask = np.array(sample[0]), np.array(sample[1])\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        #Calculamos los índices del bounding box para hacer el cropping\n",
        "        sidx=np.nonzero(mask)\n",
        "        minx=np.maximum(sidx[1].min()-self.border[1],0)\n",
        "        maxx=np.minimum(sidx[1].max()+1+self.border[1],w)\n",
        "        miny=np.maximum(sidx[0].min()-self.border[0],0)\n",
        "        maxy=np.minimum(sidx[0].max()+1+self.border[1],h)\n",
        "        #Recortamos la imagen\n",
        "        image=image[miny:maxy,minx:maxx,...]\n",
        "        mask=mask[miny:maxy,minx:maxx]\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "h70nisvTKXU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescales the image to a desired size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is the output_size.\n",
        "            If int, the smaller of the image edges is matched to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        image_np, mask_np = sample[0], sample[1]\n",
        "\n",
        "        # Get the height and width of the image\n",
        "        h, w = image_np.shape[:2]\n",
        "\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        # Resize the image using scikit-image's resize function\n",
        "        resized_image = transform.resize(image_np, (new_h, new_w))\n",
        "        resized_mask = transform.resize(mask_np, (new_h, new_w))\n",
        "\n",
        "        # Convert the resized NumPy array back to a PIL image\n",
        "        resized_image_pil = Image.fromarray((resized_image * 255).astype(np.uint8))\n",
        "        resized_mask_pil = Image.fromarray((resized_mask * 255).astype(np.uint8))\n",
        "\n",
        "        #return resized_image_pil, resized_mask_pil\n",
        "        return resized_image, resized_mask"
      ],
      "metadata": {
        "id": "IaT2mBMRKbkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CenterCrop(object):\n",
        "    \"\"\"Crop the central area of the image\n",
        "\n",
        "    Args:\n",
        "        output_size (tupla or int): Crop size. If int, square crop\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        image, mask = sample[0], sample[1]\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "        rem_h = h - new_h\n",
        "        rem_w = w - new_w\n",
        "\n",
        "        if h>new_h:\n",
        "            top = int(rem_h/2)\n",
        "        else:\n",
        "            top=0\n",
        "\n",
        "        if w>new_w:\n",
        "            left = int(rem_w/2)\n",
        "        else:\n",
        "            left = 0\n",
        "\n",
        "        image = image[top: top + new_h,\n",
        "                     left: left + new_w]\n",
        "\n",
        "        mask = mask[top: top + new_h,\n",
        "                      left: left + new_w]\n",
        "\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "ruyaJGNsKgLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km6vWZvXqqBh"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Converts the image ndarray to a tensor.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        image, mask = sample[0], sample[1]\n",
        "        # Change the axes\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        # image = np.array(image)\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbPIvTG8p9rw"
      },
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \"\"\"Normalizes the image by subtracting the mean and dividing by the standard deviations.\n",
        "\n",
        "    Args:\n",
        "        mean: The vector containing the means.\n",
        "        std: The vector containing the standard deviations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        assert len(mean) == len(std), 'Length of mean and std vectors is not the same'\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        image, mask = sample[0], sample[1]\n",
        "\n",
        "        # c, h, w = image.shape\n",
        "        # assert c==len(self.mean), 'Length of mean and image is not the same'\n",
        "\n",
        "        dtype = image.dtype\n",
        "        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n",
        "        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n",
        "        # Normalize the image\n",
        "        # image = (image - mean[:, None, None]) / std[:, None, None]\n",
        "        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnu68fiWr7Hd"
      },
      "outputs": [],
      "source": [
        "# Train ISIC Dataset------------------------------------------------------------\n",
        "train_dataset_isic = VQADataset(data_dir=database1_path,\n",
        "                           json_file='/content/data/databases_qa/ISIC_2016/qa_filtered_ISIC.json',\n",
        "                           tokenizer=None,\n",
        "                           transform=transforms.Compose([\n",
        "                            CropByMask(15),\n",
        "                            Rescale(224),\n",
        "                            CenterCrop(224),\n",
        "                            ToTensor(),\n",
        "                            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                            ]),\n",
        "                            extraPath=train_extra_ISIC_path)\n",
        "# # Test ISIC Dataset--------------------------------------------------------------\n",
        "# test_dataset_isic = VQADataset(data_dir=database1_test_path,\n",
        "#                            json_file='/content/data/databases_qa/ISIC_2016/qa_ISIC.json',\n",
        "#                            tokenizer=None,\n",
        "#                            transform=transforms.Compose([\n",
        "#                             Rescale((224,224)),\n",
        "#                             ToTensor(),\n",
        "#                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#                             ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BQq_UYpfjyg"
      },
      "outputs": [],
      "source": [
        "# Train Pizarro Dataset------------------------------------------------------------\n",
        "train_dataset_pizarro = VQADataset(data_dir=database2_path,\n",
        "                           json_file='/content/data/databases_qa/pizarro/qa_pizarro.json',\n",
        "                           tokenizer=None,\n",
        "                           transform=transforms.Compose([\n",
        "                            CropByMask(15),\n",
        "                            Rescale(224),\n",
        "                            CenterCrop(224),\n",
        "                            ToTensor(),\n",
        "                            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                            ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the all the possible answers within both datasets"
      ],
      "metadata": {
        "id": "SBodYMxwROs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all answers from the dataset\n",
        "all_answers = [answer for _, _, _, answers in train_dataset_pizarro for answer in answers]\n",
        "# Obtain unique answer values\n",
        "unique_answers = list(set(all_answers))"
      ],
      "metadata": {
        "id": "MtaTJ0v3RJ0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_answers"
      ],
      "metadata": {
        "id": "fZOPrVjkTVVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionaries id2label and label2id\n",
        "id2label = {i: label for i, label in enumerate(unique_answers)}\n",
        "label2id = {label: i for i, label in enumerate(unique_answers)}"
      ],
      "metadata": {
        "id": "zB_NjxmSSA2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "id": "8nTUep5tUe07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required tensor with the label IDs\n",
        "answers_ids_tensor = torch.arange(len(id2label), dtype=torch.float, device=device)\n",
        "answers_ids_tensor"
      ],
      "metadata": {
        "id": "zIE8BzfqUmmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrNZdQCPl7qt"
      },
      "outputs": [],
      "source": [
        "# Pizarro dataset\n",
        "pizarro_dataloader = DataLoader(train_dataset_pizarro, batch_size=5, shuffle=True)\n",
        "train_isic_dataloader = DataLoader(train_dataset_isic, batch_size=5, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jZsoFg5aoCM"
      },
      "outputs": [],
      "source": [
        "# Auxiliary function to visualize a batch\n",
        "def show_batch(sample_batched):\n",
        "    \"\"\"Show a batch of images.\"\"\"\n",
        "\n",
        "    # Store images, questions and answers\n",
        "    images_batch, masks_batch, questions_batch, answers_batch = \\\n",
        "            sample_batched[0], sample_batched[1], sample_batched[2], sample_batched[3]\n",
        "\n",
        "    # Identify the batch size to define the grid\n",
        "    batch_size = len(images_batch)\n",
        "    im_size = images_batch.size(2)\n",
        "    grid_border_size = 2\n",
        "\n",
        "    # Generate the grid\n",
        "    grid = utils.make_grid(images_batch)\n",
        "\n",
        "    # Convert to numpy and denormalize\n",
        "    grid = grid.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    grid = std * grid + mean\n",
        "    grid = np.clip(grid, 0, 1)\n",
        "    plt.imshow(grid)\n",
        "    plt.title('Batch from dataloader')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV5odxMqlhbb"
      },
      "outputs": [],
      "source": [
        "# Iterate over the dataloader and visualize batches\n",
        "for i_batch, sample_batched in enumerate(train_isic_dataloader):\n",
        "    print(i_batch, sample_batched[2])\n",
        "    show_batch(sample_batched)\n",
        "\n",
        "    # Show only the data of the 3rd batch and stop.\n",
        "    if i_batch == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQYNuFsKI3k"
      },
      "source": [
        "## Image Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL6ILzWtlPEV"
      },
      "outputs": [],
      "source": [
        "def extract_image_features(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    num_samples = len(dataloader.dataset)\n",
        "\n",
        "    # Get the output shape of the model\n",
        "    with torch.no_grad():\n",
        "        sample_input = next(iter(dataloader))[0].to(next(model.parameters()).device)\n",
        "        output_shape = model(sample_input).shape[1:]\n",
        "\n",
        "    # Calculate the total number of features\n",
        "    num_features = np.prod(output_shape)\n",
        "\n",
        "    features = np.zeros((num_samples, num_features), dtype=np.float32)\n",
        "    sample_idx = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in dataloader:\n",
        "            # Get images\n",
        "            inputs = sample[0].to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Flatten the output tensor\n",
        "            outputs_flat = outputs.view(outputs.size(0), -1)\n",
        "\n",
        "            # Store features\n",
        "            batch_size = inputs.size(0)\n",
        "            features[sample_idx:sample_idx + batch_size, :] = outputs_flat.cpu().numpy()\n",
        "            sample_idx += batch_size\n",
        "\n",
        "    return torch.tensor(features, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9gqOoAOdYQ_"
      },
      "outputs": [],
      "source": [
        "# features_pizarro = extract_image_features(resnet, pizarro_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey1rr3KGiNYm"
      },
      "outputs": [],
      "source": [
        "# features_pizarro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print GPU memory usage\n",
        "def print_gpu_memory():\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "    print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "6EeNiyfjCzIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4HfihfzJ3a6"
      },
      "source": [
        "## VQA Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, resnet, vlm, tokenizer):\n",
        "        super(VQAModel, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet model\n",
        "        self.resnet = resnet\n",
        "        # Vision Language Model\n",
        "        self.vlm = vlm\n",
        "        # Tokenizer\n",
        "        self.tokenizer = tokenizer\n",
        "        # Freeze parameters of the ResNet model\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, image_features, question, labels_idxs=None, training=False):\n",
        "\n",
        "        # image_features -> array with visual features for the specific image\n",
        "        # question -> individual question within the image batch\n",
        "        # labels_idx -> tensor with shape (0, ..., num_labels-1)\n",
        "        # training; boolean to determine the mode (train/evaluation)\n",
        "\n",
        "\n",
        "        # Prepare image inputs\n",
        "        visual_token_type_ids = torch.ones(image_features.shape[0], dtype=torch.long, device=device)\n",
        "        visual_attention_mask = torch.ones(image_features.shape[0], dtype=torch.float, device=device)\n",
        "\n",
        "        # Tokenize question\n",
        "        tokenized_question = self.tokenizer(question,\n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask=True,\n",
        "                                            add_special_tokens=True,\n",
        "                                            return_tensors='pt')\n",
        "\n",
        "        # Input for the VLM model (refer to VisualBert to understand the shapes)\n",
        "        input_dict = {\n",
        "            \"visual_embeds\": image_features.unsqueeze(0),\n",
        "            \"visual_token_type_ids\": visual_token_type_ids.unsqueeze(0),\n",
        "            \"visual_attention_mask\": visual_attention_mask.unsqueeze(0),\n",
        "            \"input_ids\": tokenized_question['input_ids'].to(device),\n",
        "            \"token_type_ids\": tokenized_question['token_type_ids'].to(device),\n",
        "            \"attention_mask\": tokenized_question['attention_mask'].to(device),\n",
        "        }\n",
        "\n",
        "        # Training mode: track gradients\n",
        "        if training:\n",
        "\n",
        "            output = self.vlm(**input_dict, labels=labels_idxs)\n",
        "            logits = output.logits\n",
        "            loss = output.loss\n",
        "\n",
        "            del output\n",
        "\n",
        "        # Evaluation mode: no gradients\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                output = self.vlm(**input_dict)\n",
        "                logits = output.logits\n",
        "\n",
        "        # # Check for memory usage\n",
        "        # print_gpu_memory()\n",
        "\n",
        "        return logits, loss"
      ],
      "metadata": {
        "id": "prExKkef7FWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-E0LJNJRyTG"
      },
      "source": [
        "### Define LLM & Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNvUgKd7R44i"
      },
      "outputs": [],
      "source": [
        "# Pre-trained base case for VQA provided in documentation\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "# bertModel = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "# Modify the visual embedding config so that size of the features matches\n",
        "configuration = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa\",\n",
        "                                                 visual_embedding_dim=512,\n",
        "                                                 num_labels=answers_ids_tensor.shape[0])\n",
        "visualBert = VisualBertForQuestionAnswering(configuration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzfYKLV5RrVi"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npW_X4I3c8dI"
      },
      "outputs": [],
      "source": [
        "# Create the VQA Model and pass it to the GPU\n",
        "vqa_model = VQAModel(resnet, visualBert, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVeBMAK4dQuQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vqa_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBFSlcm-dQiS"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrFgqLKddlJk"
      },
      "outputs": [],
      "source": [
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, vqa_model.parameters()), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all image features for the dataloader\n",
        "with torch.no_grad():\n",
        "    image_features = extract_image_features(vqa_model.resnet, pizarro_dataloader)"
      ],
      "metadata": {
        "id": "Gz8cJxSB9NYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features"
      ],
      "metadata": {
        "id": "Dq7tFNabFWxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model in training mode\n",
        "vqa_model.train()\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    text_results = []\n",
        "    for images, masks, questions, answers in tqdm(pizarro_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
        "\n",
        "        # Track batch loss\n",
        "        batch_loss = 0.0\n",
        "\n",
        "        # Transpose questions and answers\n",
        "        questions = np.transpose(questions)\n",
        "        answers = np.transpose(answers)\n",
        "        # Store shape for iteration\n",
        "        num_images, num_questions_per_image = np.shape(questions)\n",
        "\n",
        "        # Iterate over all questions for each image\n",
        "        for image_idx in range(num_images):\n",
        "          for question_idx in range(num_questions_per_image):\n",
        "\n",
        "            # Avoid using previous gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #Forward pass\n",
        "            outputs, loss = vqa_model(image_features[image_idx], questions[image_idx,question_idx], answers_ids_tensor, training=True)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss += loss.item() / (num_images * num_questions_per_image)\n",
        "\n",
        "            # Store the text predicted answers, the ground truth and corresponding question\n",
        "            probabilities = torch.softmax(outputs, dim=-1)\n",
        "            pred_idx = torch.argmax(probabilities, dim=-1)\n",
        "            pred_answer = id2label[pred_idx.item()]\n",
        "\n",
        "            text_results.append({\n",
        "                  \"Image ID\": image_idx,\n",
        "                  \"Question\": questions[image_idx,question_idx],\n",
        "                  \"Ground Truth\": answers[image_idx, question_idx],\n",
        "                  \"Prediction\": pred_answer\n",
        "                  })\n",
        "\n",
        "            # Remove unnecessary tensors from memory\n",
        "            del loss\n",
        "            del outputs\n",
        "\n",
        "        # Print batch loss\n",
        "        print(f\"Batch Loss: {batch_loss}\")\n",
        "\n",
        "        # Accumulate the total loss and num of batches\n",
        "        total_loss += batch_loss\n",
        "        num_batches += 1\n",
        "\n",
        "    # Aaverage training loss for the epoch\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n",
        "\n",
        "    # Save trained model after each epoch\n",
        "    torch.save(vqa_model.state_dict(), f'trained_vqa_model_epoch{epoch+1}.pth')"
      ],
      "metadata": {
        "id": "HeOUYMA052Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions = []\n",
        "# for image_idx in range(len(outputs)):\n",
        "#   image_answers = []\n",
        "#   for output in outputs[image_idx]:\n",
        "#       logits = output.logits\n",
        "#       probabilities = torch.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
        "#       pred_index = torch.argmax(probabilities, dim=-1)  # Get the index of the maximum probability\n",
        "#       pred_label = id2label[pred_index.item()]\n",
        "#       image_answers.append(pred_label)\n",
        "#   predictions.append(image_answers)\n",
        "# predictions"
      ],
      "metadata": {
        "id": "yT4abmfGZWT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in vqa_model.parameters():\n",
        "#       print(param.grad)"
      ],
      "metadata": {
        "id": "n7HOlCbka4uD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}